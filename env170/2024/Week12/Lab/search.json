[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Week 11 Lab: Wrong But In a Useful Way",
    "section": "",
    "text": "Introduction\nA model is something that represents an aspect of the real world by way of an analogy. A model train, for example, can represent aspects of a real locomotive like wheels and car couplings through miniature replicas. Mathematical models like the Lotka-Volterra equations can represent the dynamics of predator and prey species through the the interactions of population growth and energy. Computer simulations are a class of models that can be used to represent all kinds of phenomena, from galaxies to climate systems to potatoes.\nAll models share an important quality: they are all imperfect representations of the real phenomenon. However, despite their imperfections, they are often used to help us better understand the world around us. There is famous quote often attributed to the statistician George Box that sums up this idea:\n“All models are wrong, but some are useful.”\nModels can be useful for understanding the world in a number of ways: clarifying system dynamics, guiding data collection, suggesting new research questions, and prediction of unobserved processes or data. This lab will deal with how models, specifically linear models, can be used for predicting relationships between variables.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "Prediction.html",
    "href": "Prediction.html",
    "title": "3  Predicting new values",
    "section": "",
    "text": "Our work on Charlotte’s Willow oaks and squirrel populations caught the attention of conservation groups in South Carolina. They’d like to try and apply our model as a means of estimating tree heights in Charleston. So the question this time is: can our model, developed to estimate tree heights in Charlotte, be used to reliably estimate tree height in Charleston?\n\n\n\ncharlestoncvb.com\n\n\nWhat we are asking our model to do here is predict the heights of trees we have not measured. Our model, which is based on a a linear relationship to our predictor variables, can provide us with estimates of tree heights for any set of values for our predictor variables, so it could be used to do this.\nImmediately, though, we should be asking ourselves about how well our model is likely to perform, particularly since we are in a new place. Are there local conditions that might introduce systematic bias? For example, would differences in soil conditions affect the growth of trees in Charleston to the extent that it would affect the relationship between crown base height, DBH, and tree height? A sensible thing to do in this case would be to test the model using some Willow oaks in Charleston where the predictor variable data has already been collected and the heights of the trees is known. Luckily, such a dataset exists in our tree inventory!\nFirst, we can subset our data to the appropriate city and species:\n\ntreeDataSC&lt;-read_csv(\"data/TS3_Raw_tree_data.csv\") %&gt;%\n  filter(City==\"Charleston, SC\") %&gt;%\n  filter(CommonName==\"Willow oak\")\n\nRows: 14487 Columns: 41\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (15): Region, City, Source, Zone, Park/Street, SpCode, ScientificName, C...\ndbl (25): DbaseID, cell, Age, DBH (cm), TreeHt (m), CrnBase, CrnHt (m), Cdia...\nnum  (1): TreeID\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ntreeDataSC\n\n# A tibble: 45 × 41\n   DbaseID Region City   Source TreeID Zone  `Park/Street` SpCode ScientificName\n     &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;         &lt;chr&gt;  &lt;chr&gt;         \n 1   10638 GulfCo Charl… CHSMa…  16918 S9    Street        QUPH   Quercus phell…\n 2   10679 GulfCo Charl… CHSMa…  17680 Q9    Street        QUPH   Quercus phell…\n 3   10698 GulfCo Charl… CHSMa…  18055 R10   Street        QUPH   Quercus phell…\n 4   10705 GulfCo Charl… CHSMa…  18180 K7    Street        QUPH   Quercus phell…\n 5   10714 GulfCo Charl… CHSMa…  18445 O7    Street        QUPH   Quercus phell…\n 6   10779 GulfCo Charl… CHSMa…  19370 S10   Street        QUPH   Quercus phell…\n 7   10780 GulfCo Charl… CHSMa…  19371 S10   Street        QUPH   Quercus phell…\n 8   10844 GulfCo Charl… CHSMa…  20354 R10   Street        QUPH   Quercus phell…\n 9   10848 GulfCo Charl… CHSMa…  20431 Q9    Street        QUPH   Quercus phell…\n10   10849 GulfCo Charl… CHSMa…  20432 Q9    Street        QUPH   Quercus phell…\n# ℹ 35 more rows\n# ℹ 32 more variables: CommonName &lt;chr&gt;, TreeType &lt;chr&gt;, address &lt;chr&gt;,\n#   street &lt;chr&gt;, side &lt;chr&gt;, cell &lt;dbl&gt;, OnStreet &lt;chr&gt;, FromStreet &lt;chr&gt;,\n#   ToStreet &lt;chr&gt;, Age &lt;dbl&gt;, `DBH (cm)` &lt;dbl&gt;, `TreeHt (m)` &lt;dbl&gt;,\n#   CrnBase &lt;dbl&gt;, `CrnHt (m)` &lt;dbl&gt;, `CdiaPar (m)` &lt;dbl&gt;,\n#   `CDiaPerp (m)` &lt;dbl&gt;, `AvgCdia (m)` &lt;dbl&gt;, `Leaf (m2)` &lt;dbl&gt;,\n#   Setback &lt;dbl&gt;, TreeOr &lt;dbl&gt;, CarShade &lt;dbl&gt;, LandUse &lt;dbl&gt;, Shape &lt;dbl&gt;, …\n\n\nNext, we can use the predict function to generate some predicted tree heights for This function takes two arguments\n\nThe model from which we are deriving predictions (in this case, treeModel_dbhCb).\nA table (tibble/dataframe) that has columns with names that match the predictor variable names from our model. The important thing here is to make sure that the column names are the same between the model and new data, and that these correspond with the same kind of data.\n\n\nSCtest&lt;-predict(treeMod_cbDbh,treeDataSC)\nSCtest\n\n        1         2         3         4         5         6         7         8 \n10.138422 17.255678 20.721937 17.670871 13.544973 16.491864 23.804909 15.092028 \n        9        10        11        12        13        14        15        16 \n10.412008  9.838457  9.562108 10.685594 11.045267  9.181582 13.682479  8.988557 \n       17        18        19        20        21        22        23        24 \n16.761262  9.978725  9.645431 14.978138 13.461650 20.177528 15.368378  9.071880 \n       25        26        27        28        29        30        31        32 \n28.748802 27.407249 23.504944 10.638363 16.736309 12.502047 12.452053 12.285406 \n       33        34        35        36        37        38        39        40 \n14.651795  8.821910 20.341412 21.524606 24.214576 20.010881 18.931863 20.144199 \n       41        42        43        44        45 \n21.794005 10.685594 23.171650  6.070807  5.854166 \n\n\nThe result is a named vector, where predicted value has a number that corresponds to its row in the observed data. To evaluate our predictions, we need three pieces of information:\n\nHeights from Willow Oaks in Charleston,\nThe heights predicted for those trees by our model\nThe differences between them (also termed the residuals)\n\n\nobsHt_m&lt;-treeDataSC$`TreeHt (m)`\nmodHt_m&lt;-as.vector(SCtest)\nresiduals&lt;-obsHt_m-modHt_m\ntestData&lt;-tibble(obsHt_m,modHt_m,residuals)\n\nThe residuals are an indicator of model performance: how far off the mark is each prediction from reality? If all the residuals were 0, then there is no difference between our modeled heights and observed heights, so our model would be a perfect prediction. But with most systems of interest in the study of the environment, this is unlikely to occur: natural and social systems are almost never so straightforward. Because tree growth is influenced by a lot of factors, some deviation between model outcomes and reality is expected. But if the goal is to provide accurate predictions of tree height, we want these residuals to be as minimal as possible.\nBeyond the overall magnitude of the residuals is their organization. Ideally, we want the distribution of residuals to be normally distributed around a value of 0, which would suggest that any error in our model is due to random variation. If the residuals show some regularity in their difference, like being centered around a value other than 0 or becoming then our model is providing a poor fit to the data.\nNow let’s take a look at the predictions and their residuals from our prediction of Charleston Willow Oaks.\n\nggplot(testData,aes(x=modHt_m,y=residuals)) +\n  geom_point() +\n  geom_hline(yintercept=0,color=\"red\",lty=2) +\n  labs(x=\"Modeled height (m)\",y=\"Residuals\",title=\"Testing Willow Oak height model \\nin Charleston, SC\") +\n  theme_light()\n\n\n\n\n\n\n\n\nHow do we interpret this graph? We’ve included a red line here to indicate how far off the predictions are from reality. We can see that a good number of them fall within a range of +/- 5 meters. This isn’t terribly accurate, though it would be up to the Charleston conservation group to decide whether this is accurate enough to meet their needs.\nHowever, there’s another pattern here: the residuals from the modeled heights are systematically lower than 0; only 7 out of 45 residuals has a value above 0. We can also see this by plotting the residuals as a histogram:\n\nggplot(testData,aes(x=residuals)) +\n  geom_histogram() +\n  geom_vline(xintercept=0,color=\"red\",lty=2) +\n  labs(x=\"Predicted height error (m)\",title=\"Testing Willow Oak height model \\nin Charleston, SC\") +\n  theme_light() \n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nBecause our residuals are derived from subtracting the modeled (predicted) heights from the observed heights, negative values would indicate that modeled values are higher than the observed. This suggests that our model, based on the relationship between tree attributes in Charlotte, is systematically overestimating the heights of trees in Charleston.\nIt’s not clear why this might be the case. It could be a function of the data collection process, whereby something about the way the data was collected differed in a meaningful way. For example, the trees in Charleston may have been concentrated in one part of town, while those from Charlotte were not. In a worst-case scenario, the recording of data may have performed incorrectly in one or bother of these places, resulting in a systematic difference.\nBut assuming the data on these variables was recorded in a consistent way between these two locations, we might look to environmental differences between these two locations for an explanation. Latitude, temperature, rainfall, soil nutrients: any of these might be different between these two locations. More data, more analysis, or both may help clarify the picture. But in the meantime, to get more accurate estimates of tree heights, our friends in Charleston will likely want to develop their own model using local data.\n\n\n\n\n\n\nTry it yourself!\n\n\n\nDon’t leave Charleston hanging! Can you use the process in the previous sections to develop a model based on Willow oak trees there?\n\n\n\n3.0.1 Going further\nThis really only scratches the surface when it comes to modeling with data. Linear models are very useful, but also limited because many relationships are not linear. For example, think about the relationship between how long it would take to clean your house and the number of people helping. At lower numbers of people, the amount of time is likely to decrease as you have more assistance; but after a certain point, the house becomes too crowded, and the presence of so many people will create more mess than you can keep up with.\nOne thing to keep an eye out for is instances where the response variable is not continuously distributed across the predictor values. For example, if you have a binary response variable (0 or 1), it is not possible to consider this continuously distributed. The Generalized Linear Model can be accessed using the glm function, and allows you to specify a distribution other than normal in your assessment of the data.\nIf you’re interested in looking into doing more complex modeling, there are a number of resources out there. Here are a couple to get you started:\n\nThere is an excellent section on regression in YaRrr! The Pirate’s Guide to R by Nathaniel Phillips, including sections on interactions between predictor variables, and on using the Generalized Linear Model.\nIf you’re interested in doing more complex modeling and integrating modeling within the tidyverse workflow, I highly recommend Tidy Modeling with R by Max Kuhn and Julia Silge. The functions take on a very different structure, but once mastered can be very powerful for modeling data.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Predicting new values</span>"
    ]
  },
  {
    "objectID": "ComparingModels.html",
    "href": "ComparingModels.html",
    "title": "2  Working with regression models",
    "section": "",
    "text": "2.1 Combining variables in multiple regression\nLinear regression models can include more than one predictor term. When this is the case, th A multiple regression formula would look something like this:\ny~x1+x2+...\nIn this formula, y is still the response variable (tree height) and x1 is a predictor, x2 is a second predictor. For example, we might try combining the two predictor variables from our previous section:\ntreeMod_cbDbh&lt;-lm(`TreeHt (m)`~ `DBH (cm)` + CrnBase,data=treeDataNC)\nsummary(treeMod_cbDbh)\n\n\nCall:\nlm(formula = `TreeHt (m)` ~ `DBH (cm)` + CrnBase, data = treeDataNC)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.6417 -2.7392  0.5231  2.5024  8.8383 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  6.31802    1.11825   5.650 9.66e-07 ***\n`DBH (cm)`   0.16665    0.01571  10.608 6.02e-14 ***\nCrnBase      0.88047    0.20309   4.335 7.85e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.589 on 46 degrees of freedom\nMultiple R-squared:  0.8195,    Adjusted R-squared:  0.8116 \nF-statistic: 104.4 on 2 and 46 DF,  p-value: &lt; 2.2e-16\nThis gives a marginal improvement on our last model. What about a third predictor? How about parallel crown diameter?\ntreeMod_cbDbhDia&lt;-lm(`TreeHt (m)`~ CrnBase + `DBH (cm)` + `CdiaPar (m)` ,data=treeDataNC)\nsummary(treeMod_cbDbhDia)\n\n\nCall:\nlm(formula = `TreeHt (m)` ~ CrnBase + `DBH (cm)` + `CdiaPar (m)`, \n    data = treeDataNC)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.6352 -2.6076  0.5357  2.4097  8.8775 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    6.19594    1.21140   5.115 6.27e-06 ***\nCrnBase        0.87649    0.20565   4.262 0.000102 ***\n`DBH (cm)`     0.15823    0.03410   4.641 3.01e-05 ***\n`CdiaPar (m)`  0.04966    0.17799   0.279 0.781500    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.625 on 45 degrees of freedom\nMultiple R-squared:  0.8198,    Adjusted R-squared:  0.8078 \nF-statistic: 68.23 on 3 and 45 DF,  p-value: &lt; 2.2e-16\nHmmm… the multiple R-squared went up slightly, but the adjusted went down. What gives? An important point to consider here is that adding variables adds to model complexity. When adding predictors, the name of the game is trying to balance model simplicity with explanatory power. In this case, the additional variability explained by the added predictor is offset in the adjusted R-squared by the added complexity of the model. Adding in a ton of variables may cause what is called “overfitting”, where the model is fit so perfectly to the dataset it is built from that it wouldn’t do well in predicting other values.\nIt is also important not to lose sight of the goal of the modeling venture. Here, we want to provide a good way to estimate tree height that is easier than measuring the height itself. Requiring multiple measurements may not achieve this goal, so the tradeoff may not be worth it to the user.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Working with regression models</span>"
    ]
  },
  {
    "objectID": "ComparingModels.html#comparing-models",
    "href": "ComparingModels.html#comparing-models",
    "title": "2  Working with regression models",
    "section": "2.2 Comparing models",
    "text": "2.2 Comparing models\nWhat if we want to look at the relative fit of two models? We can do this visually by checking the model’s fitted values, those it predicts, versus those that are observed. For example, here’s what this looks like for the crown base height model:\n\ntrueHeight&lt;-treeDataNC$`TreeHt (m)`\nmodHeight&lt;-treeMod_cb$fitted.values\nfitData&lt;-tibble(trueHeight,modHeight)\n\nggplot(fitData,aes(x=trueHeight,y=modHeight))+\n  geom_point() +\n  geom_abline(b = 1, a = 0) +\n  labs(x=\"Observed Tree Height (m)\",y=\"Modeled Tree Height (m)\") +\n  theme_light()\n\nWarning in geom_abline(b = 1, a = 0): Ignoring unknown parameters: `b` and `a`\n\n\n\n\n\n\n\n\n\nIdeally, if the model does a good job of estimating reality, our estimated values should fall along a diagonal line with a slope of 1 and a y-intcercept of 0, indicating a perfect linear relationship. Here, we use geom_abline function to show that line in our graph, and how the observed heights relate to their modeled counterparts.\nIn this case, the points don’t align well with the line, consistent with the R2 we saw for this model earlier indicating a poor fit. Now let’s look at the model based on DBH:\n\ntrueHeight&lt;-treeDataNC$`TreeHt (m)`\nmodHeight&lt;-treeMod_dbh$fitted.values\nfitData&lt;-tibble(trueHeight,modHeight)\n\nggplot(fitData,aes(x=trueHeight,y=modHeight))+\n  geom_point() +\n  geom_abline() +\n  labs(x=\"Observed Tree Height (m)\",y=\"Modeled Tree Height (m)\") +\n  theme_light()\n\n\n\n\n\n\n\n\nBetter. And now adding the two\n\ntrueHeight&lt;-treeDataNC$`TreeHt (m)`\nmodHeight&lt;-treeMod_cbDbh$fitted.values\nfitData&lt;-tibble(trueHeight,modHeight)\n\nggplot(fitData,aes(x=trueHeight,y=modHeight))+\n  geom_point() +\n  geom_abline() +\n  labs(x=\"Observed Tree Height (m)\",y=\"Modeled Tree Height (m)\") +\n  theme_light()\n\n\n\n\n\n\n\n\nSimilar to our last model, consistent with the finding of a marginal improvement. We can also use the Analysis of Variance (ANOVA) tests to compare two models. Here, we’ll compare the model using crown base model with the combined crown base-DBH model:\n\nanova(treeMod_cb, treeMod_cbDbh)\n\nWarning in anova.lmlist(object, ...): models with response '\"TreeHt (m)\"'\nremoved because response differs from model 1\n\n\nAnalysis of Variance Table\n\nResponse: treeDataNC$`TreeHt (m)`\n                   Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ntreeDataNC$CrnBase  1 1239.8 1239.82  28.541 2.62e-06 ***\nResiduals          47 2041.7   43.44                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe RSS or residual sum of squares on the second model and the low p-value indicates an improvement between model 1 and model 2. Now let’s compare cbDBH and cbDBHDia:\n\nanova(treeMod_cbDbh,treeMod_cbDbhDia)\n\nAnalysis of Variance Table\n\nModel 1: `TreeHt (m)` ~ `DBH (cm)` + CrnBase\nModel 2: `TreeHt (m)` ~ CrnBase + `DBH (cm)` + `CdiaPar (m)`\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     46 592.40                           \n2     45 591.37  1    1.0232 0.0779 0.7815\n\n\nIn this case, the residual sum of squares went up with the addition of the new predictor, and thus the p-value is well above the 0.05 cutoff.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Working with regression models</span>"
    ]
  },
  {
    "objectID": "BuildingModels.html",
    "href": "BuildingModels.html",
    "title": "1  A motivating example",
    "section": "",
    "text": "Let’s say that the city of Charlotte, NC wants to protect critical habitats in its urban forests. A recent study has shown that the endangered Carolina northern flying squirrel has a preference for Willow Oak trees above a height of 25 meters. The city parks department works with local volunteers to identify these trees; however, taking accurate height measurements can be difficult and time-consuming. They would like to find an easier way to estimate height, so they turn to you, the trusty data scientist, to try and answer this question.\nThe question we want to answer is: can we use the relationship between tree height and some other measurement(s) to estimate, or model, tree height? So first, we would want to know whether a relationship exists between measurements.\nFirst, let’s get some tree data into R:\n\nlibrary(tidyverse) \n\ntreeData&lt;-read_csv(\"data/TS3_Raw_tree_data.csv\") \n\ntreeDataNC&lt;-treeData %&gt;%\n  filter(City==\"Charlotte, NC\") %&gt;%\n  filter(CommonName==\"Willow oak\")\n\ntreeDataNC\n\n# A tibble: 49 × 41\n   DbaseID Region City   Source TreeID Zone  `Park/Street` SpCode ScientificName\n     &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;         &lt;chr&gt;  &lt;chr&gt;         \n 1   11435 Piedmt Charl… CLTMa…  10225 F6    Street        QUPH   Quercus phell…\n 2   11439 Piedmt Charl… CLTMa…  11053 F6    Street        QUPH   Quercus phell…\n 3   11447 Piedmt Charl… CLTMa…  12222 F6    Street        QUPH   Quercus phell…\n 4   11462 Piedmt Charl… CLTMa…  13521 F6    Street        QUPH   Quercus phell…\n 5   11469 Piedmt Charl… CLTMa…  14232 F6    Street        QUPH   Quercus phell…\n 6   11503 Piedmt Charl… CLTMa…  19141 F5    Street        QUPH   Quercus phell…\n 7   11514 Piedmt Charl… CLTMa…  20011 F5    Street        QUPH   Quercus phell…\n 8   11539 Piedmt Charl… CLTMa…  23048 F6    Street        QUPH   Quercus phell…\n 9   11543 Piedmt Charl… CLTMa…  23565 E8    Street        QUPH   Quercus phell…\n10   11570 Piedmt Charl… CLTMa…  27540 F8    Street        QUPH   Quercus phell…\n# ℹ 39 more rows\n# ℹ 32 more variables: CommonName &lt;chr&gt;, TreeType &lt;chr&gt;, address &lt;chr&gt;,\n#   street &lt;chr&gt;, side &lt;chr&gt;, cell &lt;dbl&gt;, OnStreet &lt;chr&gt;, FromStreet &lt;chr&gt;,\n#   ToStreet &lt;chr&gt;, Age &lt;dbl&gt;, `DBH (cm)` &lt;dbl&gt;, `TreeHt (m)` &lt;dbl&gt;,\n#   CrnBase &lt;dbl&gt;, `CrnHt (m)` &lt;dbl&gt;, `CdiaPar (m)` &lt;dbl&gt;,\n#   `CDiaPerp (m)` &lt;dbl&gt;, `AvgCdia (m)` &lt;dbl&gt;, `Leaf (m2)` &lt;dbl&gt;,\n#   Setback &lt;dbl&gt;, TreeOr &lt;dbl&gt;, CarShade &lt;dbl&gt;, LandUse &lt;dbl&gt;, Shape &lt;dbl&gt;, …\n\n\nLet’s start with crown base height (CrnBase in the data): this is the average distance from the ground to the bottom of the tree’s crown:\n\n\n\nFerraz et al. 2009 The Role of Lidar Systems in Fuel Mapping\n\n\nWhat we’d like to know is whether a linear relationship exists between crown base height and tree height. By linear, we mean that an increase of some increment in one variable (crown base height) will coincide with in an increase in the variable of interest (tree height). Let’s see what the relationship between our variables looks like visually:\n\nggplot(treeDataNC,aes(x=CrnBase,y=`TreeHt (m)`)) +\n  geom_point() +\n  labs(x=\"Crown Base (m)\",y=\"Tree Height (m)\") +\n  theme_light()\n\n\n\n\n\n\n\n\nNotice CrnBase is not surrounded by backticks, while TreeHt (m) has them. Remember that the reason is that CrnBase doesn’t have any non-standard characters like whitespace or parentheses, so the read_csv function didn’t add them, and they aren’t needed to refer to the variable in the aesthetic mapping.\nOK, so what do we see here? Well, there does seem to be an increase, but it isn’t very strong. We can assess this strength by doing a correlation test:\n\n#check pearson\ncor.test(treeDataNC$CrnBase,treeDataNC$`TreeHt (m)`,method=\"pearson\")\n\n\n    Pearson's product-moment correlation\n\ndata:  treeDataNC$CrnBase and treeDataNC$`TreeHt (m)`\nt = 5.3424, df = 47, p-value = 2.62e-06\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.4031591 0.7638432\nsample estimates:\n      cor \n0.6146721 \n\n\nThe cor value is 0.61, which suggests a positive relationship, but it isn’t especially strong. The p-value is very low, though, so this indicates we can reject the idea there isn’t a relationship. Of course, we didn’t check to see whether the data are normally distributed. We can use shapiro.test to do that:\n\n#check normality\nshapiro.test(treeDataNC$`TreeHt (m)`)\n\n\n    Shapiro-Wilk normality test\n\ndata:  treeDataNC$`TreeHt (m)`\nW = 0.96854, p-value = 0.2117\n\nshapiro.test(treeDataNC$CrnBase)\n\n\n    Shapiro-Wilk normality test\n\ndata:  treeDataNC$CrnBase\nW = 0.84429, p-value = 1.293e-05\n\n\nRemembering back to Week 6, a p-value greater than our cut-off threshold (0.05) tells us that we can’t reject the null hypothesis that the data are normally distributed. This is true for tree height, but not so for the crown base height. Therefore, we should probably run our correlation test again, but this time using the Spearman method:\n\n#check spearman\ncor.test(treeDataNC$CrnBase,treeDataNC$`TreeHt (m)`,method=\"spearman\")\n\n\n    Spearman's rank correlation rho\n\ndata:  treeDataNC$CrnBase and treeDataNC$`TreeHt (m)`\nS = 6156.7, p-value = 5.37e-08\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.6858807 \n\n\nThe result is pretty similar: we can reject the idea that there is no relationship, and the data indicate a positive relationship. The rho value is a little improved, but not by a lot.\n\n1.0.1 Building a model: single predictor regression\nGiven the existence of a relationship, we could ask: how well can we predict tree height from crown base height? A linear model can help us to estimate this. If you think back to our scatter plot, what a linear model does is draw a straight line through the data that minimizes the distance to all of the points.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nIf we find that the line fits the data well, which would be indicated by how closely the data fall along it, then we can use the line to provide us with predictions about data we have not observed. For example, looking at the plot above, the linear model would predict that a tree with a crown base height of 10 meters would have a total height of about 31 meters. Of course, the value of this prediction depends on how well the model fits the data.\nTo create our model, we’ll use the lm, or linear model, function:\n\ntreeModel&lt;-lm(`TreeHt (m)`~ CrnBase,data=treeDataNC)\n\nWhat have we done here? What this code does is asks R to conduct a linear regression on these two variables and save it as a linear model object called treeModel. At a minimum, lm needs an argument that comes in the form of a formula:\ny~x\nWhere y is the response variable (tree height) and x is a predictor (crown base height). If we just want to use column names like we have here, we also have to supply a data argument, which in this case is the treeDataNC tibble. We could get the same result without this argument by referring to the columns using the $ operator:\n\ntreeMod_cb&lt;-lm(treeDataNC$`TreeHt (m)`~ treeDataNC$CrnBase)\n\nSo now our model is stored as treeMod_cb. We can get a quick look at the results by using the summary function:\n\nsummary(treeMod_cb)\n\n\nCall:\nlm(formula = treeDataNC$`TreeHt (m)` ~ treeDataNC$CrnBase)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.7496  -4.3383   0.9595   5.5553  11.1581 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         12.8348     1.7161   7.479 1.54e-09 ***\ntreeDataNC$CrnBase   1.8014     0.3372   5.342 2.62e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.591 on 47 degrees of freedom\nMultiple R-squared:  0.3778,    Adjusted R-squared:  0.3646 \nF-statistic: 28.54 on 1 and 47 DF,  p-value: 2.62e-06\n\n\nThere’s a few different pieces of information here, let’s go through some of the most relevant:\n\nResiduals: Residuals are how far the real data deviate from the line produced by the model. These are expressed here as an interquartile range\nCoefficients: These are indicating the relative increments that the response variable changes in response to the predictors.\nR-squared (R2): These are measures of the proportion of variance in the response (dependent) variable accounted for by the model A value of 1 would indicate a perfect match between predictor and\n\nMultiple R-squared is the raw R-squared; for instances where more than one predictor is used, this\nAdjusted R-squared adjusts for the number of predictors, favoring models with fewer predictors. Under most circumstances, this is the preferred\n\n: This is\n\nAn adjusted R-squared value of 0.36 is not great. Let’s see if we can get a better match with diameter at breast height, or DBH:\n\nggplot(treeDataNC,aes(x=`DBH (cm)`,y=`TreeHt (m)`)) +\n  geom_point() +\n  geom_smooth(method=\"lm\",se=FALSE) +\n  labs(x=\"Diameter at Breast Height (cm)\",y=\"Tree Height (m)\") +\n  theme_light()\n\n\n\n\n\n\n\n\nHere you might notice is that we added an argument, method=\"lm\", to the geom_smooth function. This is saying that, , we want to use a linear model. This lets ggplot create a line from a linear model rather than use the default loess smoother. We also included the argument se=false, which removes the error ribbon around the line. This allows us to see the model as it would exist from the formula we used above.\nFrom this plot, it looks like there is a pretty good relationship between these two variables. However, we still want to assess the data using a correlation:\n\n#check normality\nshapiro.test(treeDataNC$`TreeHt (m)`)\n\n\n    Shapiro-Wilk normality test\n\ndata:  treeDataNC$`TreeHt (m)`\nW = 0.96854, p-value = 0.2117\n\nshapiro.test(treeDataNC$`DBH (cm)`)\n\n\n    Shapiro-Wilk normality test\n\ndata:  treeDataNC$`DBH (cm)`\nW = 0.9492, p-value = 0.03429\n\n#check spearman\ncor.test(treeDataNC$`DBH (cm)`,treeDataNC$`TreeHt (m)`,method=\"spearman\")\n\nWarning in cor.test.default(treeDataNC$`DBH (cm)`, treeDataNC$`TreeHt (m)`, :\nCannot compute exact p-value with ties\n\n\n\n    Spearman's rank correlation rho\n\ndata:  treeDataNC$`DBH (cm)` and treeDataNC$`TreeHt (m)`\nS = 2571.2, p-value = 5.943e-16\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n     rho \n0.868814 \n\n\nThe Spearman test says we can’t rule out a relationship (no surprise), and there is a stronger positive relationship than what we saw before (again, not surprising).\nNow let’s see how this performs as a model:\n\ntreeMod_dbh&lt;-lm(`TreeHt (m)`~`DBH (cm)`,data=treeDataNC)\nsummary(treeMod_dbh)\n\n\nCall:\nlm(formula = `TreeHt (m)` ~ `DBH (cm)`, data = treeDataNC)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.0051 -2.0312 -0.0645  2.0496 13.3311 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  8.24141    1.20525   6.838 1.44e-08 ***\n`DBH (cm)`   0.19576    0.01667  11.740 1.41e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.214 on 47 degrees of freedom\nMultiple R-squared:  0.7457,    Adjusted R-squared:  0.7403 \nF-statistic: 137.8 on 1 and 47 DF,  p-value: 1.411e-15\n\n\nThis gives us an improved R-squared, suggesting that more of the variability in this dataset is explained by the model.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A motivating example</span>"
    ]
  }
]