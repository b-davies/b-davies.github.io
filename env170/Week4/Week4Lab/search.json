[
  {
    "objectID": "01_WhyDoEDA.html",
    "href": "01_WhyDoEDA.html",
    "title": "1  Introducing Exploratory Data Analysis",
    "section": "",
    "text": "library(modeldata)\nlibrary(palmerpenguins)\n\n\nAttaching package: 'palmerpenguins'\n\n\nThe following object is masked from 'package:modeldata':\n\n    penguins\n\nlibrary(datasets)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nOne of the main reasons data science exists is that there is an enormous amount of data available for study that have already been collected or are actively being generated. Data collected by researchers, governments, corporations, and even the public at large are growing every day, and this offers tremendous opportunities to combine, remix, and re-use data to gain insights about the world around us. But this comes with a caveat: secondary data users have no control over how the data are generated, and so begin\n\nIs this data useful for a given purpose?\nIs the quality of this data sufficient to use?\nDoes this data show potentially meaningful patterning?\n\nExploratory Data Analysis is an approach to evaluate the composition and structure within a dataset prior to formal modeling or hypothesis testing. In this process, we use visualizations, transformations, and statistical summaries to systematically look at data and identify where interesting or relevant patterning lies.\nThere are no set rules for how to conduct this kind of analysis. Instead, there are a wide array of guidelines and techniques you might apply depending on what kind of data you have and what aspects of the data you are interested in. In the rest of this section, we’ll look at what kinds of data we might expect to encounter, and then look at them individually.\n\nKnowing Your Data\nA first step towards exploring data is to understand what kind of data you’re dealing with. As we discussed in Week 3, different kinds of data will be better represented by different kinds of visualizations. But they will also be amenable to different kinds of analyses, and have different pecularities.\nHave a look at the first few rows of the olympics dataset:\n\nolympics&lt;-read_csv(\"data/olympics.csv\")\n\nRows: 39783 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (6): Name, Sex, NOC, Season, Sport, Medal\ndbl (4): Age, Height, Weight, Year\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nolympics\n\n# A tibble: 39,783 × 10\n   Name                 Sex     Age Height Weight NOC    Year Season Sport Medal\n   &lt;chr&gt;                &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;\n 1 Edgar Lindenau Aabye M        34     NA     NA DEN    1900 Summer Tug-… Gold \n 2 Arvo Ossian Aaltonen M        30     NA     NA FIN    1920 Summer Swim… Bron…\n 3 Arvo Ossian Aaltonen M        30     NA     NA FIN    1920 Summer Swim… Bron…\n 4 Juhamatti Tapio Aal… M        28    184     85 FIN    2014 Winter Ice … Bron…\n 5 Paavo Johannes Aalt… M        28    175     64 FIN    1948 Summer Gymn… Bron…\n 6 Paavo Johannes Aalt… M        28    175     64 FIN    1948 Summer Gymn… Gold \n 7 Paavo Johannes Aalt… M        28    175     64 FIN    1948 Summer Gymn… Gold \n 8 Paavo Johannes Aalt… M        28    175     64 FIN    1948 Summer Gymn… Gold \n 9 Paavo Johannes Aalt… M        32    175     64 FIN    1952 Summer Gymn… Bron…\n10 Kjetil Andr Aamodt   M        20    176     85 NOR    1992 Winter Alpi… Gold \n# ℹ 39,773 more rows\n\n\nWe can see a lot of different kinds of data here. Some of it is numbers, some of it is character values. In previous sessions, we’ve talked about these as numerical and categorical data. But even these categories are broad: there are different kinds of numerical data here, and different kinds of categories. A more detailed way to distinguish these kinds of data is in terms of level of measurement, which are nominal, ordinal, interval, and ratio. We’ll go through each one of these individually so we know what we’re looking at.\n\nNominal data\nNominal data is data without a meaningful distance or order between values. In other contexts we’ve referred to this as categorical data, which means that that the differences between values It can also be thought of as discrete, because it exists within a finite set of values.\nIn the olympics dataset, there are several columns containing nominal data. These include:\n\nName\nSex\nNOC, which stands for National Olympic Committee\n\nIf we want to look at this nominal data, we have a few options, but our most trusted tool is the bar plot, which will show us the number of each category.\n\nggplot(olympics,aes(x=Sex)) +\n  geom_bar()\n\n\n\n\nBy adding another aesthetic (like fill color), we can make a stacked bar to look at more than one nominal variable.\n\nggplot(olympics,aes(x=Sex,fill=Season)) +\n  geom_bar()\n\n\n\n\nAlready, we’re starting to see patterns. There are far more male than female medalists in this dataset, and overall there fewer Winter than Summer medals. A quick look at the history of the games can get us the information we need. For the first pattern, there were historically fewer events in the Olympic games.\nFind more statistics at  Statista\nFor the second, the Winter Olympics has only been held for about half as many years as the Summer Games, and has historically had fewer medaling events overall.\n\n\nOrdinal\nLike nominal data, ordinal data is categorical, but unlike nominal data it has a logical order. This ordering is a usable source of information for analytic purposes.\nIn our olympics dataset, there is only one true ordinal variable, and that’s the Medal column, whereby the medal is indicative of the rank of the athlete in the competition, with the ranking Gold &gt; Silver &gt; Bronze.\nWhat kind of graphing\nPeculiarities\n\n\nInterval and Ratio\nInterval and ratio data are examples of numerical data, . This means that the distances between values have meaning.\nInterval data are refers to data with meaningful\nUsually, the presence (or possibility) of negative values is a dead giveaway that you’re dealing with interval data.\nHistogram or similar. You might be (rightly) asking at this stage in the course whywe use histograms? By counting plots\nWhat it looks like\nWhat kind of graphing: Histogram or similar"
  },
  {
    "objectID": "02_LookingForPatterns.html",
    "href": "02_LookingForPatterns.html",
    "title": "2  Looking for Patterns",
    "section": "",
    "text": "library(modeldata)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nWhen we start looking for patterning in the data, we’\n\n2.0.1 Univariate patterns\n\n2.0.1.1 Counts\n--bar charts\nWhat if the counts are already in the data?\n\nhairEye&lt;-as_tibble(HairEyeColor)\nggplot(data=hairEye,aes(x=Hair,y=n,fill=Eye)) + \n  geom_col()\n\n\n\n\n\n\n2.0.1.2 Ranks\n\n\n2.0.1.3 Distributions\n-Center\n--Mean\n--Median\n-Spread: Spread refers to how\n--Range\n--Standard Deviation\n-Shape\n--Normal\n--Bimodal, multimodal\n---Density plots\nTRY IT YOURSELF: Look at X dataset\n\n\n\n2.0.2 Bivariate\n\n2.0.2.1 Categorical and categorical\n\n\n2.0.2.2 \n\nhairEye&lt;-as_tibble(HairEyeColor)\nggplot(data=hairEye,aes(x=Hair,y=n,fill=Eye)) + \n  geom_col()\n\n\n\n\n\n\n2.0.2.3 Categorical and numerical\nLike histograms, boxplots show the center, spread, and shape of distributions, but these are most useful when comparing more than one group.\nMedian: the central line of the boxplot is\nInterquartile range: The “box” part of the box is an indication of the interquartile range. This is a\nHave a look at the Sacramento dataset:\n\nSacramento\n\n# A tibble: 932 × 9\n   city           zip     beds baths  sqft type        price latitude longitude\n   &lt;fct&gt;          &lt;fct&gt;  &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;       &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 SACRAMENTO     z95838     2     1   836 Residential 59222     38.6     -121.\n 2 SACRAMENTO     z95823     3     1  1167 Residential 68212     38.5     -121.\n 3 SACRAMENTO     z95815     2     1   796 Residential 68880     38.6     -121.\n 4 SACRAMENTO     z95815     2     1   852 Residential 69307     38.6     -121.\n 5 SACRAMENTO     z95824     2     1   797 Residential 81900     38.5     -121.\n 6 SACRAMENTO     z95841     3     1  1122 Condo       89921     38.7     -121.\n 7 SACRAMENTO     z95842     3     2  1104 Residential 90895     38.7     -121.\n 8 SACRAMENTO     z95820     3     1  1177 Residential 91002     38.5     -121.\n 9 RANCHO_CORDOVA z95670     2     2   941 Condo       94905     38.6     -121.\n10 RIO_LINDA      z95673     3     2  1146 Residential 98937     38.7     -121.\n# ℹ 922 more rows\n\n\n\nggplot(data=Sacramento,aes(x=type,y=price)) +\n  geom_boxplot()\n\n\n\n\n\n\n2.0.2.4 Two numerical\n-Scatterplots\nForm: Is\nDirection: If increasing values on one axis correspond with increasing values on the other, then the direction is positive; if increasing values on\nStrength:\n\n\n\n\n\n\nTry it yourself!\n\n\n\n\n\n\n\n\n\n2.0.3 Multivariate\nWith more than two variables,\nDoes my data show signs of clustering?\nDoes a pattern exist for some categories by not others?"
  },
  {
    "objectID": "02_OneVariable.html",
    "href": "02_OneVariable.html",
    "title": "2  Looking for Patterns in One Variable",
    "section": "",
    "text": "library(modeldata)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nolympics&lt;-read_csv(\"data/olympics.csv\")\n\nRows: 39783 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (6): Name, Sex, NOC, Season, Sport, Medal\ndbl (4): Age, Height, Weight, Year\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThere’s no right or wrong way to do EDA, but many often start simply by exploring patterns in single variables. This is called univariate analysis. We do this\n\nNominal/Ordinal\nWhat we’re looking for here are relative proportions. Does some categories stand out from the others? We’ve seen the geom_bar geometry be used for this. But what if the counts are already in the data? For example, look at the HairEyeColor dataset from the modeldata package:\n\nhairEye&lt;-as_tibble(HairEyeColor)\nhairEye\n\n# A tibble: 32 × 4\n   Hair  Eye   Sex       n\n   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;\n 1 Black Brown Male     32\n 2 Brown Brown Male     53\n 3 Red   Brown Male     10\n 4 Blond Brown Male      3\n 5 Black Blue  Male     11\n 6 Brown Blue  Male     50\n 7 Red   Blue  Male     10\n 8 Blond Blue  Male     30\n 9 Black Hazel Male     10\n10 Brown Hazel Male     25\n# ℹ 22 more rows\n\n\nThis is data on hair and eye color recorded in a statistics class. Here, we can see columns giving the categories of hair and eye color, as well as sex, while the counts for each combination are stored in the n variable (shorthand for number). Normally, the geom_bar function counts each instance for us, so if we use that here, it will just count each time a category is mentioned in that column:\n\nggplot(data=hairEye,aes(x=Hair)) + \n  geom_bar()\n\n\n\n\nNot especially helpful. There are a few different ways we can deal with this, but one way is to use the geom_col function:\n\nggplot(data=hairEye,aes(x=Hair,y=n)) + \n  geom_col()\n\n\n\n\nThis function makes two aesthetic mappings: the variable of interest on the x-axis (in this case, hair color), and the number of instances as a column-stored variable on the y-axis.\n\n\nDistributions\nComing back to the question at the end of the last section: why histograms? By counting the number of instances in different bins across the range of values, the histogram ends up approximating a probability distribution:\n\n\n\n\n\nA probability distribution is a way of visualizing the probability that any individual measurement in a population will produce a given value. Let’s dive into this using the olympics data:\n\nggplot(olympics,aes(x=Weight)) +\n  geom_histogram(bins=20) \n\nWarning: Removed 9327 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\nThere are several things we might look at when we’re visualizing data using a histogram:\nCenter\nWhere is the average, or most likely, value. Two common measures are mean (the sum of the values divided by their number) and median (the middle-most value):\n\nmean(olympics$Weight,na.rm=TRUE)\n\n[1] 73.77068\n\nmedian(olympics$Height,na.rm=TRUE)\n\n[1] 178\n\n\nSpread\nA common measure is the standard deviation, which takes the square root of the average of the squared deviations, or distances from the mean.\n\nsd(olympics$Weight,na.rm=TRUE)\n\n[1] 15.01602\n\nsd(olympics$Height,na.rm=TRUE)\n\n[1] 10.89372\n\n\nSkew\nSkew indicates whether the data are evenly spread around the center or have values that are notably larger (positive skew) or smaller (negative skew). For example, if we look at heights among male Olympians:\n\nolympicsM&lt;-olympics[olympics$Sex==\"M\",]\n\nggplot(olympicsM,aes(x=Height)) +\n  geom_histogram(bins=20) \n\nWarning: Removed 7936 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\nWe can see these are pretty evenly distributed around an average height value. If we look at weight, on the other hand:\n\nggplot(olympicsM,aes(x=Weight)) +\n  geom_histogram(bins=20) \n\nWarning: Removed 8426 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\nWe see notable positive skew in these values, indicated by the x-axis carrying on to the right."
  },
  {
    "objectID": "04_WhenThingsLookWeird.html",
    "href": "04_WhenThingsLookWeird.html",
    "title": "4  What To Do With Outliers",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(modeldata)\nabalone&lt;-read_csv(\"data/abalone.csv\")\n\nNew names:\nRows: 4177 Columns: 10\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(1): sex dbl (9): ...1, length, diameter, height, weight.whole, weight.shucked,\nweigh...\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n\nSo far, most of the example data we have dealt have been selected because they fit the purpose of demonstrating a concept. But real data seldom comes without some kind of issue, problem, or weirdness. People constantly make data entry mistakes, data collection and storage protocols are rarely standardized, and data often include cases that, real or not, fly in the face of our expectations.\nConducting EDA means keeping a critical eye out for these kinds of things, and being able to make judgement calls about when a value is an appropriate outlier and when it is truly weird. In this section, we’ll look at how we deal with this.\n\nOutliers and unusual values\nAs we mentioned in our discussion of visualizing categorical and numerical data using boxplots, outliers are indicated using points:\n\nggplot(data=Sacramento,aes(x=type,y=price)) +   \n  geom_boxplot()\n\n\n\n\nWhen we see these outliers, our first question should be: do these values make sense? Besides the house prices being shockingly out-of-date (the median house price in Sacamento is closer to $470,000 now), having home prices in the vicinity of $750,000 doesn’t seem wildly out of character. And if we do some digging on house prices, we know that house prices are often positively skewed, with some prices being substantially higher than the average:\n\n\n\nhttps://www.urban.org/urban-wire/bostons-housing-market-three-charts\n\n\nSo these outliers don’t seem super crazy and probably reflect what we\n\nggplot(data=abalone,aes(x=height*200,y=weight.shucked*200)) +\n  geom_point() +\n  geom_smooth()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\nIs it real? Does it make sense? How can we tell? To do this, we’ll have to think a little critically about what our data represent: the dimensions of shellfish. Abalone are bivalves that are shaped more or less like oblong discs.\n\nggplot(data=abalone,aes(x=height*200,y=length*200)) +\n  geom_point()\n\n\n\n\nWe can see our outliers on the right hand side. For the tallest one, which has a height somewhere around 225 mm, it is only showing a length of around 90 mm. This means that this abalone is taller than it is wide, which would make for a very strange looking shellfish.\nWhat might explain this? It seems possible that the person recording the data made a mistake. Looking at the data, we can see that most other abalones recorded in this dataset with a height of around 90 mm have a height of about 25 mm. It’s entirely possible that whoever entered this data added an extra 2 when typing it in.\nUnfortunately, it’s not good practice to second-guess what someone meant to enter. It could have been 25, or 22, or something else that they meant to type all together. So rather than change the data to a value with think is correct, the safer thing to do is to eliminate that observation. We can use the square brackets to do this:\n\nabalone2&lt;-abalone[abalone$height*200&lt;150,]\nggplot(data=abalone2,aes(x=height,y=weight.shucked)) +\n  geom_point() +\n  geom_smooth()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\nHowever, when you start removing data it is EXTREMELY important that you document that you are doing so, and explain why. Comments in your script are a good place to do this.\n\n#Removing outlier due to suspect height value (too high for length)\nabalone2&lt;-abalone[abalone$height*200&lt;150,]\n\nggplot(data=abalone2,aes(x=height,y=weight.shucked)) +\n  geom_point() +\n  geom_smooth()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTry it yourself!\n\n\n\nTake a good look at that other outlier in the heights (on the far right of the last plot). Does it make sense? Why or why not?"
  },
  {
    "objectID": "05_WhatsNext.html",
    "href": "05_WhatsNext.html",
    "title": "5  What’s Next",
    "section": "",
    "text": "Trendlines\nStatistical tests\nThe approaches outlined here are just a handful of the more common techniques that data scientists use to tease out information about datasets. These have arisen through practice as ways of addressing regularly-occurring issues that come up . As you begin to work with data more regularly, you might develop your own techniques to address issues within your own domain. If you find a solution to a problem that someone else might find useful,"
  },
  {
    "objectID": "06_LabExercise4.html",
    "href": "06_LabExercise4.html",
    "title": "5  Lab Exercise 4",
    "section": "",
    "text": "Public Domain, https://commons.wikimedia.org/w/index.php?curid=205407\n\n\nIn 2012, a researcher in coastal California was looking for ways to better assess populations of large mammalian predators for conservation purposes. Their solution? To determine how effectively morphometric traits (shape and size measurements) from animal dung could be used to distinguish particular species. To do this, they collected samples and identified the species using DNA, and then conducted a series of measurements on each sample like length and diameter, as well as shape characteristics such as tapering and segmentation.\nYour academic supervisor came across this dataset and wants to know what sorts of things you would look for in this data before they go off to spend the summer in the woods collecting animal scat. Like a good data scientist, you’re going to do a bit of EDA to take a look at the data first.\nUsing the predatorScat.csv data file, produce a script that demonstrates how you would look at this data. In particular, I want you to be able to show:\n\nLook at 3 different kinds of data in a univariate analysis, and describe the kinds of patterning you see\nHow you would look at 2 different kinds of relationships in a bivariate analysis (e.g., two numerical, two categorical, numerical-categorical), and describe the patterning you see\nIdentify any outliers in the data and whether they appear to be genuine or suspect\n\nWrite your findings as comments in the script alongside the code used to produce the graph and submit the script on Canvas."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Week 4 Lab: Exploratory Data Analysis in R",
    "section": "",
    "text": "Introduction\nLast week we looked at visualizing data, emphasizing how visualization can be used to reveal patterns that are not obvious in raw data. But how do we begin to look at data? What kinds of things should we be looking for? And how does this help guide our ability to analyze and tell stories with data?\nThis week we’ll build on our visualization skills by employing them in Exploratory Data Analysis. EDA is one of the first steps taken in any data science project. It is used to reveal patterns and problems in data, and help to solidify the research questions, analytical methods, and visualization techniques you will use. But EDA is less a formal set of procedures and more like a dialogue you have with the data; the actual procedures you undertake will depend on what you are looking for and what you find as you go. In this lab, you will learn about:\n\nCommon data types, their properties, and how they are visualized\nWhat kinds of descriptive patterning to look for when evaluating data visually\nHow to manage true and anomalous outliers\n\nFor this lab, we’ll be looking at a number of different datasets. You will need to download the Week4Data.zip folder, unzip it, and then save the contents to the appropriate place in your file system. We’ll also be using the following packages, which may need to be installed for your use here:\n\n#install.packages(\"datasets\")\n#install.packages(\"modeldata\")"
  },
  {
    "objectID": "03_TwoPlusVariables.html",
    "href": "03_TwoPlusVariables.html",
    "title": "3  Looking for Patterns in More than One Variable",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(modeldata)\n\nThe most interesting and useful patterns are often those that involve more than one variable. These allow us to posit a relationship that may help us to explain the cause of a given\nWhen we look at two variables together, it’s called bivariate analysis, and when we look at more than two we usually use the term multivariate.\n\nCategorical and categorical\n\n\n\n\n\nCategorical and numerical\nLike histograms, boxplots show the center, spread, and shape of distributions, but these are most useful when comparing more than one group. These usually have four main components:\n\nCenter line The central line of the boxplot is the median value.\nBox The “box” part of the box is an indication of the interquartile range: the values between the 25% and 75% quartiles. Half of our data falls within the range defined by the bounds of the box.\nWhiskers The whiskers are the lines at either end of the boxplot. These extend to 10% and 90%, respectively.\nOutliers Boxplots use individual points to visualize outliers, which is anything that falls beyond the reach of the whiskers.\n\nHave a look at the Sacramento dataset from the modeldata package:\n\n#|\nSacramento\n\n# A tibble: 932 × 9\n   city           zip     beds baths  sqft type        price latitude longitude\n   &lt;fct&gt;          &lt;fct&gt;  &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;       &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 SACRAMENTO     z95838     2     1   836 Residential 59222     38.6     -121.\n 2 SACRAMENTO     z95823     3     1  1167 Residential 68212     38.5     -121.\n 3 SACRAMENTO     z95815     2     1   796 Residential 68880     38.6     -121.\n 4 SACRAMENTO     z95815     2     1   852 Residential 69307     38.6     -121.\n 5 SACRAMENTO     z95824     2     1   797 Residential 81900     38.5     -121.\n 6 SACRAMENTO     z95841     3     1  1122 Condo       89921     38.7     -121.\n 7 SACRAMENTO     z95842     3     2  1104 Residential 90895     38.7     -121.\n 8 SACRAMENTO     z95820     3     1  1177 Residential 91002     38.5     -121.\n 9 RANCHO_CORDOVA z95670     2     2   941 Condo       94905     38.6     -121.\n10 RIO_LINDA      z95673     3     2  1146 Residential 98937     38.7     -121.\n# ℹ 922 more rows\n\n\nLots of variables to look at here, including nominal (city, type), interval (latitude, longitude), and ratio (beds, sqft, price). To get a quick sense of what things are looking like, we can use a boxplot to examine the relationship between a nominal variable (type) and a ratio variable (price):\n\nggplot(data=Sacramento,aes(x=type,y=price)) +   \n  geom_boxplot()\n\n\n\n\nFirst, there’s a good deal of overlap between these different categories. However, the mean values are greater for multifamily and residential-type housing than they are for condominiums. Finally, there are a fair number of of outliers in the residential-type, indicating a positive skew in the data. We’ll come back to this a bit later. By viewing these boxplots side-by-side, we’re turning our univariate view of price into a bivariate view by type.\n\n\nTwo numerical\nAs we’ve seen before, scatterplots are a common way to look at the relationship between two numerical (ratio/interval) variables. When we use scatterplots in EDA, we’r looking for some general kinds of patterns that indicate certain kinds of relationships. To illustrate this, we’ll use the abalone dataset you downloaded:\n\nabalone&lt;-read_csv(\"data/abalone.csv\")\n\nNew names:\nRows: 4177 Columns: 10\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(1): sex dbl (9): ...1, length, diameter, height, weight.whole, weight.shucked,\nweigh...\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\nabalone\n\n# A tibble: 4,177 × 10\n    ...1 sex   length diameter height weight.whole weight.shucked weight.viscera\n   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt;          &lt;dbl&gt;          &lt;dbl&gt;\n 1     1 M      0.455    0.365  0.095        0.514         0.224          0.101 \n 2     2 M      0.35     0.265  0.09         0.226         0.0995         0.0485\n 3     3 F      0.53     0.42   0.135        0.677         0.256          0.142 \n 4     4 M      0.44     0.365  0.125        0.516         0.216          0.114 \n 5     5 I      0.33     0.255  0.08         0.205         0.0895         0.0395\n 6     6 I      0.425    0.3    0.095        0.352         0.141          0.0775\n 7     7 F      0.53     0.415  0.15         0.778         0.237          0.142 \n 8     8 F      0.545    0.425  0.125        0.768         0.294          0.150 \n 9     9 M      0.475    0.37   0.125        0.509         0.216          0.112 \n10    10 F      0.55     0.44   0.15         0.894         0.314          0.151 \n# ℹ 4,167 more rows\n# ℹ 2 more variables: weight.shell &lt;dbl&gt;, rings &lt;dbl&gt;\n\n\nThese are metrics on black-lipped abalone from Tasmania, used to evaluate the relationship between physical size and age (as determined by growth rings). This data has a number of ratio values, but it’s not immediately what the some of the units are just by looking: given the average length of an abalone shell (80-130mm), they don’t really work for most of the common kinds of measurement units we might encounter (millimeters, centimeters, inches). The original data were actually re-scaled from millimeters and grams to use in a machine-learning exercise by dividing them all by 200. We could do a wholesale conversion of these values, but since we’re exploring we’ll just do this when we do our aesthetic mapping.\nPatterns that we might be looking for in the shape of a scatterplot are:\n\nDirection An increase in one variable generally corresponds with a change, either increase (positive direction) or decrease (negative direction), in the other.\nForm The change seems to follow a line (linear) or a curve (curvilinear)\nStrength The slope of a change is either steep (strong) or shallow (weak)\n\nLet’s have a look at the abalone data to see what we mean. First, we’ll look at the relationship between shell diameter and the number of growth rings:\n\nggplot(data=abalone,aes(x=diameter*200,y=rings)) +\n  geom_point() +\n  labs(x=\"Diameter (mm)\",y=\"Number of Rings\")\n\n\n\n\nWhat do you notice that’s a little strange about this data? It seems to be organized in horizontal lines. Both of our values (diameter and number of rings) are ratio data, but one of them (number of rings) is an integer and therefore discrete. This means that gaps exist between our values, creating the visual effect of horizontal lines.\nWhat else do we see about this relationship? It seems as though the number of rings increases with increasing diameter, suggesting a positive direction. We can visualize this more clearly by adding a line that tries to minimize the distance between itself and all the points using the geom_smooth function:\n\nggplot(data=abalone,aes(x=diameter*200,y=rings)) +\n  geom_point() +\n  geom_smooth()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\nThe geom_smooth function adds the line along with a shaded envelope indicating Now let’s look at the relationship between diameter and whole weight of the shellfish:\n\nggplot(data=abalone,aes(x=diameter,y=weight.whole)) +\n  geom_point()\n\n\n\n\nIn this case, the weight increases along with the diameter, but the rate of increase is not consistent; it seems to increase with diameter. On a basic level, we could say that there is a curvilinear relationship here, which we can illustrate again with a smoothed line:\n\nggplot(data=abalone,aes(x=diameter*200,y=weight.whole*200)) + geom_point() +\n  geom_smooth()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\n\n\nTry it yourself!\n\n\n\nLook at these relationships in the abalone data. How would you describe them?\n\nDiameter and length\nWhole-weight and rings\nRings and shucked-weight"
  }
]